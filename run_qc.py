#!/usr/bin/env python

################################################################################
#	CONFIG SECTION
################################################################################

#Edit default values in this section

defaults: dict[str:str] = {
	#Default BUSCO lineage to use
	"busco":   "eukaryota_odb10",
	#Name of file to write statistics to
	"outfile": "stats.tsv",
	#Name of directory to write generated GFF files to
	"outdir":  "gff_out"
}



################################################################################
#	IMPORTS
################################################################################

#GFF parsing
from pandas import DataFrame
#Argument parsing
from argparse import ArgumentParser, RawDescriptionHelpFormatter
#Used in counting various things
from collections import Counter
#Dealing with file paths properly
from pathlib import Path
#Various helper functions
from qc_func import *
#File management
from file_ledger import *
#Used for retrieving the directory of the script
from os.path import dirname, realpath
#Inline stat calculation
from statistics import mean
#Type hints
from typing import *



################################################################################
#	DEPENDENCY CHECKS
################################################################################

#Check that each dependency is in PATH
dependencies: list[str] = [
		"xtractore",
		"intersectBed",
		"busco",
		"makeblastdb",
		"blastn"
	]

for dep in dependencies:
	require_exec(dep)


################################################################################
#	STATS & PATHS SETUP
################################################################################

#Ledger to keep track of all the relevant files
paths = PathLedger()

#Directory of the script
paths.script_d = dirname(realpath(__file__))
#Root of reference data directory
paths.species_d = ( paths.script_d / "refs", PathReq.IS_DIR )
#Directory for temporary files
paths.tmp_d = paths.script_d / "tmp"


#Names of species which have reference data available
species: set[str] = set(
		subdir.name for subdir in paths.species_d.iterdir() if subdir.is_dir()
	)

#Check that there actually is a species
if len(species) == 0:
	error(f"No per-species reference data found in {paths.species_d}")


#This dictionary will hold the final statistics
stats: dict[str:Any] = dict()

#This dictionary will hold statistics pertaining to the reference annotation
#These statistics aren't reported, but need to be kept track of for calculations
ref_stats: dict[str:Any] = dict()



################################################################################
#	ARGUMENT PARSING
################################################################################

args = ArgumentParser()

#GFF file with annotation to assess
args.add_argument("gff", metavar="GFF",
	help = "GTF/GFF file with annotation")

#Species name
args.add_argument("species", metavar="SPECIES",
	help ="Name of species whose reference data to use")

#FASTA with protein sequences
args.add_argument("fasta", metavar="FASTA", nargs='?',
	help = "FASTA file with protein sequences")

args.epilog = "Valid values for SPECIES are: " + ", ".join(species)


#Annotation name
args.add_argument("-n", "--name",
	help = "Name to save the results under, in place of an autogenerated one")

#BUSCO lineage
args.add_argument("-b", "--busco", metavar="LINEAGE",
	help = "BUSCO lineage to use instead of " + defaults["busco"],
	default = defaults["busco"])

#Whether to use CDSs over exons
args.add_argument("-C", "--cds",
	help = "Use CDS features in place of exons",
	action = "store_true")

#Output file (for statistics)
args.add_argument("-o", "--outfile", metavar="FILE",
	help = "File to write statistics to",
	default = defaults["outfile"])

#Output directory (for GFF file)
args.add_argument("-O", "--outdir", metavar="DIR",
	help = "Directory to write generated GFF to",
	default = defaults["outdir"])


#Parse, validate and propagate arguments as needed
args = args.parse_args()


#Input GFF & FASTA
paths.annot = (args.gff,   PathReq.IS_FILE)
if args.fasta:
	paths.prot  = (args.fasta, PathReq.IS_FILE)


#Species
if args.species not in species:
	error(f"No reference data found for species {args.species}")

#Path to reference data of the requested species
#There's no need to check if this directory exists, since args.species's value
#was just checked to be in range
paths.ref_d = paths.species_d / args.species

#Reference data files
paths.genome     = (paths.ref_d / "genome.fa", PathReq.IS_FILE)
paths.ref_annot  = (paths.ref_d / "annot.gff", PathReq.IS_FILE)
paths.ref_exon   = paths.ref_d / "exon.fa"
paths.ref_intron = paths.ref_d / "intron.fa"


#Get name for the annotation, if it was not passed with --name
stats["name"] = args.name if args.name else paths.annot.absolute().parent.name


#Feature type to use: exons or CDSs
args.ft = "CDS" if args.cds else "exon"


#Output paths
paths.outfile = args.outfile
paths.outdir = args.outdir



################################################################################
#	DATA PREPARATION
################################################################################
#Create temporary file directory as necessary
paths.tmp_d.mkdir(exist_ok=True)


eprint("Loading annotation")

#Deserialize annotation
annot: Gff = get_gff(paths.annot)

#Harmonize "mRNA" and "transcript" feature types between GFF and GTF format
gff_harmonize(annot)

#Save the harmonized GFF to a new file, to be used in sequence extraction
paths.tmp_gff = paths.tmp_d / "tmp.gff"
gff_save(annot, paths.tmp_gff)



eprint("Extracting exon & intron sequences")

#Filenames for annotation exon & intron sequences
paths.ann_exon   = paths.tmp_d / "exon.fa"
paths.ann_intron = paths.tmp_d / "intron.fa"

#Sequence extraction proper
run_xtractore(paths.tmp_gff, paths.genome, args.ft,  paths.ann_exon)
run_xtractore(paths.tmp_gff, paths.genome, "intron", paths.ann_intron)


#The reference exon/intron FASTAs need to be generated if they do not exist,
#and regenerated if the reference annotation is newer - check if that's the case

#Get modification times of each file, with a safeguard in case of missing files
ref_annot_mtime:  float = get_mtime(paths.ref_annot)
ref_exon_mtime:   float = get_mtime(paths.ref_exon)
ref_intron_mtime: float = get_mtime(paths.ref_intron)

#This condition evaluates to True if ref_annot is newer than ref_exon, or
#ref_exon does not exist
if ref_annot_mtime > ref_exon_mtime:
	#(Re)generate exon FASTA
	run_xtractore(paths.ref_annot, paths.genome, "exon", paths.ref_exon)

#Analogously for introns
if ref_annot_mtime > ref_intron_mtime:
	run_xtractore(paths.ref_annot, paths.genome, "intron", paths.ref_intron)



#Make BLAST databases
eprint("Creating BLAST databases")

#Filenames for BLAST databases
#Technically, these paths are not for individual files, but for sets of files
#which make up BLAST databases
paths.blastdb_exon   = paths.tmp_d / "exon"
paths.blastdb_intron = paths.tmp_d / "intron"

run_makeblastdb(paths.ann_exon,   paths.blastdb_exon)
run_makeblastdb(paths.ann_intron, paths.blastdb_intron)




################################################################################
#	BASE STATISTICS
################################################################################
eprint("Calculating base statistics") 


#Number of genes
stats["gene_cnt"] = gff_ftcnt(annot, "gene")
#Proportion of sub-gene features to genes
stats["trans_ratio"]  = gff_ftcnt(annot, "mRNA")   / stats["gene_cnt"]
stats["exon_ratio"]   = gff_ftcnt(annot, args.ft)  / stats["gene_cnt"]
stats["intron_ratio"] = gff_ftcnt(annot, "intron") / stats["gene_cnt"]

#Average lengths of features
stats["gene_len"]   = gff_ftlen(annot, "gene")
stats["exon_len"]   = gff_ftlen(annot, args.ft)
stats["intron_len"] = gff_ftlen(annot, "intron")


#Deserialize reference annotation
ref_annot: Gff = get_gff(paths.ref_annot)

#Get feature counts in reference annotation
ref_stats["gene_cnt"]   = gff_ftcnt(ref_annot, "gene")
ref_stats["exon_cnt"]   = gff_ftcnt(ref_annot, "exon")
ref_stats["intron_cnt"] = gff_ftcnt(ref_annot, "intron")


################################################################################
#	BUSCO
################################################################################
#Determine whether BUSCO should be run
if args.fasta:
	eprint("Running BUSCO")
	
	
	#Directory for holding BUSCO results
	paths.busco_d = paths.tmp_d / "busco"
	#Run BUSCO
	run_busco(paths.prot, args.busco, paths.busco_d)
	
	#Deserialize BUSCO results
	busco: dict = get_busco(paths.busco_d)
	
	
	#Add BUSCO percentages to statistics
	stats["busco_s"] = busco["results"]["Single copy percentage"]
	stats["busco_d"] = busco["results"]["Multi copy percentage"]
	stats["busco_f"] = busco["results"]["Fragmented percentage"]
	stats["busco_m"] = busco["results"]["Missing percentage"]

else:
	#If protein FASTA was not passed, push dummy values as BUSCO stats
	stats["busco_s"] = ""
	stats["busco_d"] = ""
	stats["busco_f"] = ""
	stats["busco_m"] = ""


################################################################################
#	BLAST
################################################################################
eprint("Running BLAST")


#Paths for tabular BLAST results
paths.blast_exon   = paths.tmp_d / "exon.tsv"
paths.blast_intron = paths.tmp_d / "intron.tsv"

#Run BLAST
run_blastn(paths.ref_exon,   paths.blastdb_exon,   paths.blast_exon)
run_blastn(paths.ref_intron, paths.blastdb_intron, paths.blast_intron)


#Deserialize results
blast_exon:   DataFrame = get_blast(paths.blast_exon)
blast_intron: DataFrame = get_blast(paths.blast_intron)


#Get statistics based on BLAST results

#Number of reference exons/introns that got a hit
stats["exon_hits"]   = blast_qcnt(blast_exon)
stats["intron_hits"] = blast_qcnt(blast_intron)
#Divide by total number of reference exon/introns to get a proportion
stats["exon_hits"]   /= ref_stats["exon_cnt"]
stats["intron_hits"] /= ref_stats["intron_cnt"]
#Turn proportions into percentages
stats["exon_hits"]   *= 100
stats["intron_hits"] *= 100

#Median E-value
stats["exon_eval"]   = blast_eval(blast_exon)
stats["intron_eval"] = blast_eval(blast_intron)
#Average %identity
stats["exon_pident"]   = blast_pident(blast_exon)
stats["intron_pident"] = blast_pident(blast_intron)
#Average query coverage
stats["exon_qcov"]   = blast_qcov(blast_exon)
stats["intron_qcov"] = blast_qcov(blast_intron)
#Average subject coverage
stats["exon_scov"]   = blast_scov(blast_exon)
stats["intron_scov"] = blast_scov(blast_intron)


################################################################################
#	GFF GENERATION
################################################################################
eprint("Generating GFF file")


paths.outdir.mkdir(exist_ok=True)
#Path for to-be-generated GFF
paths.out_gff = paths.outdir / ( stats["name"] + ".gff" )

#Genes from the input annotation which are in the same genomic region as any
#gene from the reference annotation are copied to the generated GFF file,
#whereas input annotation genes which do not overlap any reference annotation
#genes are not included in the output
#A single pass with intersectBed would copy over genes, but not necessarily all
#children features of some genes. To remedy this, two passes are applied

#First, the reference annotation is used as the matrix for extraction of
#features from the input annotation
#This will copy over all relevant genes, but not necessarily all of their
#children features
run_intersectBed(paths.annot, paths.ref_annot, paths.tmp_gff)

#The GFF just generated will contain all of the relevant genes, each of which
#will naturally overlap all of its children features
#As such, this just-generated GFF can be used as the matrix for extraction,
#again from the input annotation, to extract all relevant genes exhaustively
run_intersectBed(paths.annot, paths.tmp_gff, paths.out_gff)


#Count number of genes present in generated file
stats["gene_frag"] = gff_ftcnt(get_gff(paths.out_gff), "gene")
#Divide by number of genes in reference annotation to get a measure of gene
#fragmentation
stats["gene_frag"] /= ref_stats["gene_cnt"]


################################################################################
#	SEQUENCE-BASED STATISTICS
################################################################################

eprint("Calculating sequence-based statistics")


#Deserialize exon & intron sequences for both reference data and annotation
ref_exon:   Fasta = get_fasta(paths.ref_exon)
ann_exon:   Fasta = get_fasta(paths.ann_exon)
ref_intron: Fasta = get_fasta(paths.ref_intron)
ann_intron: Fasta = get_fasta(paths.ann_intron)


#GC content
stats["exon_gccont"]   = fasta_gccont(ann_exon)
stats["intron_gccont"] = fasta_gccont(ann_intron)

#Count perfect matches between both sets of exons/introns
stats["exon_matches"]   = fasta_matchcnt(ref_exon,   ann_exon)
stats["intron_matches"] = fasta_matchcnt(ref_intron, ann_intron) 
#Turn counts into proportions
stats["exon_matches"]   /= ref_stats["exon_cnt"]
stats["intron_matches"] /= ref_stats["intron_cnt"]
#Turn proportions into percentages
stats["exon_matches"]   *= 100
stats["intron_matches"] *= 100


#Splice site distribution
stats["splice_sites"] = fasta_ssdist(ann_intron)


################################################################################
#	CONCLUSION
################################################################################

#Check if stats file exists already, to tell if the header needs to be included
outfile_exists: bool = paths.outfile.is_file()

#Open file for writing
outfile: IO = open(paths.outfile, 'a')


#Write header if necessary
if not outfile_exists:
	outfile.write( '\t'.join( stat_name for stat_name in stats.keys() ) )
	outfile.write('\n')

#Write stat values
outfile.write( '\t'.join( str(stat_value) for stat_value in stats.values() ) )
outfile.write('\n')


eprint(f"Finished. See results in {paths.outfile} and {paths.out_gff}.")
